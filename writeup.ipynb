{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tsne663 import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing t-SNE\n",
    "\n",
    "**Contributors: Marc Brooks, Rob Kravec, Steven Winter**\n",
    "\n",
    "## Abstract\n",
    "**To be done**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background [1, 2]\n",
    "In practice, high-dimensional data is often concentrated near a lower dimensional manifold. This can occur for a variety of reasons, but is commonly viewed as a signal that the underlying data-generating mechanism depends on significantly fewer parameters than the dimension of the observed data. Consider image data summarizing a single individual's face at a random distance and angle: as an array, each image could be extremely high dimensional, despite most of the variability depending on only two parameters. \n",
    "\n",
    "Recovering the underlying manifold structure is desirable in some applications and necessary in others. Data visualization is fundamentally easier in lower dimensional spaces; more accurate visualizations allow an analyst to exploit patterns in the data during modelling and inference. Computationally, low-dimensional data takes less memory to store, and is always faster to process or summarize with standard algorithms. Mathematically, low-dimensional data is unafflicted by the curse of dimensionality. For all of these reasons, the results of low-dimensional inference or modelling are generally more accurate and informative than inference performed on the original data. For example, consider analyzing images of a baseball player mid-throw. Each image could be viewed as an array in some larger space, say $\\mathbb{R}^{N\\times N}$. Interpolating between two such images in Euclidean space (e.g., drawing a line between the two arrays) would produce nonsensical, physically imposible results. One must learn the underlying image manifold, and then interpolate in that space.\n",
    "\n",
    "Formally, learning an underlying manifold is necessary when the true data generating mechanism involves sampling points $y_i\\in \\mathbb{R}^d$, $i=1,...,n$ from some manifold $M\\subseteq \\mathbb{R}^d$, and then embedding these points into ambient space via a function $f:M\\to \\mathbb{R}^D$, $D>d$. The analyst observes $x_i = f(y_i)$ and needs to construct estimates $\\widehat y_i$ of $y_i$. Different methods of manifold learning place different assumptions on $f$. For example, principal component analysis (PCA) assumes $f$ is linear, constructs the $d$ dimensional plane $M$ that explains most of the variability in the observed data, and takes $\\widehat y_i$ to be the projection of $x_i$ onto that plane. PCA is fast, but fails to handle nonlinear embeddings. An alternative is Isomap, which assumes $f$ is an isometry (i.e. distance preserving), and thus tries to choose $\\widehat y_i$ such that the distance between $\\widehat y_i$ and $\\widehat y_j$ is close to the distance between $x_i$ and $x_j$. Intuitively, points that are far apart in the ambient space should be sent to points that are far apart in the low-dimensional space, and points that are close together should stay close together. The key step in Isomap is to build a graph by defining an edge $x_ix_j$ with weight $||x_i-x_j||$ if $||x_i-x_j||$ is small (and no edge if this is large). Distances between distance points $x_k$, $x_\\ell$ are estimated as the shortest path along this graph, and then multidimensional scaling is used to find a linear embedding which preserves these distances. Isomap is still quite fast, but fails when $f$ is not an isometry (e.g., when $f$ is only conformal). Furthermore, Isomap is only guaranteed to produce reliable estimates when the true $y_i$ are sampled uniformly from $M$.\n",
    "\n",
    "Symmetric stochastic neighbor embedding (S-SNE) is another approach to nonlinear manifold learning which instead attempts to preserve a notion of probabilistic similarities between points. In the ambient space the data are assumed to be jointly Gaussain; a similarity matrix is constructed using the conditional probability that $x_j$ came from a Gaussian centered as $x_i$. S-SNE searches for low-dimensional $\\widehat y_i$ that also come from a Gaussian distribution and preserve the similarity matrix from the ambient space, where \"preserve\" is measured using the Kullbackâ€“Leibler (KL) divergence. S-SNE struggles when one point has many neighbours all at a similar distance: for example, assume $x_1$ is at the center of a cirle and all $x_j$, $j>1$ live on this circle. Embedding the $x_i$ into one dimension would force many (very different) $x_j$ onto the same points. Naive solutions such as modifying the modifying the KL-divergence penalty result in horribly slow optimization problems. In this document, we implement and study t-distributed stochastic neighbor embedding (t-SNE), which replaces the Gaussian assumption on $y_i$ with a t-distribution assumption. A heavier tailed distribution forces points at moderate distances in the ambient space to be embedded very far apart, and thus ameliorates crowding. Our t-SNE implementation is based on Laurens van der Maaten and Geoffrey Hinton's 2008 paper, \"Visualizing Data using t-SNE.\" \n",
    "\n",
    "\n",
    "In particular, we implement t-SNE in basic python, and then significantly improve our implementation using numpy broadcasting, just-in-time (JIT) numba C++ compilation, and parallelization. Each version of our implementation is thoroughly benchmarked. The fastest versions of our implementation are then tested on simualted data, the MNIST digits dataset used in our reference paper, and two additional biology-related datasets. Finally, we compare our implementation to PCA and Isomap on difficult nonlinear simulations as well as the real datasets previously mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Algorithm [1, 3, 4]\n",
    "\n",
    "### Preliminaries\n",
    "The algorithm begins by converting high-dimensional data $x_1,...,x_n$ into an $n\\times n$ matrix of conditional probabilities. These conditional probabilities measure the similarity between points under a pre-specified Gaussian distribution. More formally, consider a multivariate Gaussian distribution centered at a single point $x_i$ with isotropic variance $\\sigma_i^2$; let $f_i$ be the probability density function for this distribution. The conditional probability $p_{j|i}$ is the probability that $x_j$ would pick $x_i$ as its neighbour (out of all $\\{x_1,...,x_n\\}\\setminus \\{x_j\\}$ if neighbours were chosen randomly with probabilities $f_i(x_j)$. In symbols:\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i-x_j||^2/2\\sigma_i^2)}{\\sum_{k\\neq i}\\exp(-||x_i-x_k||^2/2\\sigma_i^2)}.\n",
    "$$\n",
    "Conditional probabilities are converted to joint probabilities via\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2n}.\n",
    "$$\n",
    "If $x_i$ and $x_j$ are close together from the perspective of a Gaussian distribution, then $p_{ij}$ will be large. If they are far apart, then $p_{ij}$ will be small. In this sense $\\sigma_i^2$ controls what \"close together\" means; these variances are chosen so that the entropy equals a user specified value called the perplexity:\n",
    "$$\n",
    "\\text{Perp}(\\sigma_1,...,\\sigma_n) = 2^{-\\sum_j p_{j|i}\\log_2p_{i|j}}.\n",
    "$$\n",
    "Low-dimensional points $y_1,...,y_d$ are modelled with a $t$ distribution, wherein the joint probabilities are\n",
    "$$\n",
    "q_{ij} = \\frac{(1+||y_i-y_j||)^{-1}}{\\sum_{k\\neq \\ell}(1+||y_k-y_\\ell||)^{-1}}.\n",
    "$$\n",
    "Intuitively, we want the distribution of the $y_i$ to \"look similar\" to the distribution of the $x_i$. The KL-divergence measures similarity between probability distributions; as such t-SNE aims to minimize\n",
    "$$\n",
    "KL(P||Q) =\\sum_{i, j}p_{ij}\\log\\bigg(\\frac{p_{ij}}{q_{ij}}\\bigg).\n",
    "$$\n",
    "One can compute the gradient directly as\n",
    "$$\n",
    "\\frac{dKL(P||Q)}{dy_i} = 4\\sum_{j}\\frac{p_{ij}-q_{ij}}{(1+||y_i-y_j||)^{-1}}(y_i-y_j).\n",
    "$$\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "We now have everything we need to state the main algorithm.\n",
    "\n",
    "---\n",
    "**Algorithm 1:** t-SNE  \n",
    "**Inputs:**  \n",
    "High dimensional data - $X = \\{x_1,...,x_n\\}$,  \n",
    "Desired perplexity - $Perp$,  \n",
    "Number of iterations - $T$,  \n",
    "Learning rate - $\\eta$,  \n",
    "Momentum - $\\alpha(t)$  \n",
    "**Outputs:**  \n",
    "Low-dimensional latent factors - $Y = \\{y_1,...,y_n\\}$.\n",
    "\n",
    "**Begin:**\n",
    "1. Optimize $\\sigma_i$ to the desired perplexity $Perp$ (e.g. with binary search).\n",
    "2. Compute $p_{ij}$ using $X$.\n",
    "3. Sample initial solution $Y^{(0)}\\sim N(0, 10^{-4})$ entrywise.\n",
    "4. For $t=0,...,T$:\n",
    "  1. Compute $q_{ij}$ using $Y^{(0)}$.\n",
    "  2. Compute $dKL(P||Q)/dY$ using $Y^{(0)}$.\n",
    "  3. Set $Y^{(t)}=Y^{(t-1)}+\\eta dKL(P||Q)/dY - \\alpha(t)(Y^{(t-1)}-Y^{(t-2)})$  \n",
    "  \n",
    "**End**\n",
    "\n",
    "---\n",
    "\n",
    "For completeness, we also outline binary search.\n",
    "\n",
    "---\n",
    "**Algorithm 2:** Binary search  \n",
    "**Inputs:**  \n",
    "Evaluation function - $f$,  \n",
    "Target value - $target$,  \n",
    "Tolerance - $tol$,  \n",
    "Number of iterations - $T$,  \n",
    "Lower bound - $LB$,  \n",
    "Upper bound - $UB$,  \n",
    "**Outputs:**  \n",
    "Estimated solution $x$ to $f(x)=target$.\n",
    "\n",
    "**Begin:**\n",
    "1. For $t=1,...,T$ or until converged:\n",
    "  1. Set $guess = (LB+UB)/2$.  \n",
    "  2. Compute $val=f(guess)$.\n",
    "  3. If $val > target$, set $upper=guess$. Otherwise set $lower=guess$.\n",
    "  \n",
    "**End**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Accessories\n",
    "Overflow is a concern when computing $||x_i-x_j||$ in high dimensions. As such, we augment our implementation to run PCA whenever the dimension of the input data is larger than $30$. t-SNE is run on the resulting $30$-dimensional linear factors.\n",
    "\n",
    "The above algorithm offers significant gains over S-SNE, but can nonetheless be improved with a small number of tricks. The first trick is known as exaggeration: For the first several (say $50$) iterations of t-SNE, we use $4p_{ij}$ instead of $p_{ij}$. The $q_{ij}$, which always sum to $1$, will never be able to adequately model these exaggerated joint probabilities. Consequently, the cost function forces all of the $q_{ij}$ to model the largest probabilities; this has the effect of forming tight widely seperated clusters early on, which makes it easier for the clusters to move around each other once the exaggeration has been removed.\n",
    "\n",
    "Another trick is to use a learning rate schedule, $\\eta(t)$, instead of a constant $\\eta$. Adaptive learning rates accelerate training in the early stages of the algorithm (which compounds the effects of exaggeration), as well as reduce the risk of choosing an inappropriate learning rate for the application at hand. Our implementation of t-SNE allows for custom learning rate functions (as long as $\\eta_t$ depends only on $t$, $\\eta_{t-1}$, $\\eta_0$, and constants), but also comes with the following default schedules:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{Constant:}\\quad \\eta_{t+1} = c, \\quad c>0, \\\\\n",
    "&\\text{Time-based:}\\quad \\eta_{t+1} = \\frac{\\eta_t}{1+dt}, \\quad d>0, \\\\\n",
    "&\\text{Step-based:}\\quad \\eta_{t+1} = \\eta_0d^{\\text{floor}(1+t)/r}, \\quad d>0, r>0, \\\\\n",
    "&\\text{Exponential :}\\quad \\eta_{t+1} = \\eta_0\\exp(-dn), \\quad d>0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "These parameters control how the learning rate decays with time. For example, a step-based decay with $d=1/3$ and $r=10$ corresponds to dividing the learning rate by $3$ every $10$ iterations. We choose a small momentum term for the first few iterations (e.g. $\\alpha(t)=0.5$ for $t<250$). Once the data are reasonably well seperated, increasing the momentum allows us to speed toward an extreme point (e.g. $\\alpha(t)=0.2$ for $t\\geq 250$).\n",
    "\n",
    "Finally, our implementation allows one to modify the distribution of the embedded data to incorporate heavier tails. The original implementation of t-SNE uses a t-distribution with one degree of freedom to model the embedded $y_i$; Kobak et al. [3] show that replacing the standard t-distribution kernel with $\\nu$ degrees of freedom\n",
    "$$\n",
    "p(d|\\nu) = \\frac{1}{(1+d^2/\\nu)^{(\\nu+1)/2}}\n",
    "$$\n",
    "with the scaled kernel \n",
    "$$\n",
    "p(d|\\lambda) = \\frac{1}{(1+d^2/\\lambda)^{\\lambda}}\n",
    "$$\n",
    "also yields a simple, closed-form update for the gradient - namely\n",
    "$$\n",
    "\\frac{dKL(P||Q)}{dy_i} = 4\\sum_{j}\\frac{p_{ij}-q_{ij}}{(1+||y_i-y_j||/\\lambda)^{-1}}(y_i-y_j).\n",
    "$$\n",
    "As $\\lambda \\to \\infty$, the embedded distribution converges to a Gaussian and we recover S-SNE. When $\\lambda=1$, this is regular t-SNE. The most interesting region is $\\lambda\\in (0, 1/2)$, which corresponds to a theoretical t-distribution with $-1$ to $0$ degrees of freedom. This allows for extremely heavy tails, which in turn reveals within-cluster structures not visible with standard t-SNE. For example, [3] shows $\\lambda=0.5$ splits MNIST clusters into groupings based on writing style: one sees $5$ groups of $1$s based on the slant, as well as several groups of $4$s based on whether or not the top portion is closed. We do not have the computational resources to reproduce their experiments, but nonetheless wanted to implement this generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "### Opportunities for Improvement\n",
    "\n",
    "Profiling our initial Python implementation revealed three main bottlenecks: optimizng $\\sigma_i$ to the target perplexity, computing pairwise distances between the $y_i$, and computing the gradient. Pairwise distances can be computed efficiently with broadcasting; ptimizing variances and computing the gradient can be done in parallel. Our main function, `tsne`, accepts an argument `optim` which can take values in {`\"none\"`, `\"fast\"`, `\"fastest\"`} that correspond to combinations of these improvements (as well as other, less impactful changes such as decorating with `@jit`). Key differences between these choices are listed below.\n",
    "\n",
    "**Basic Python:**  \n",
    "* Pairwise distances are computed in a double `for` loop\n",
    "* Gradient is computed one entry at a time in a triple `for` loop.\n",
    "\n",
    "**Vectorization and Scikit-learn:**\n",
    "\n",
    "* Pairwise distances use sklearn's `pairwise` function.\n",
    "* Each row of the gradient is written as a broadcasted dot product with numpy.\n",
    "\n",
    "**Numba and Parallelization:**\n",
    "\n",
    "* Same `PCA`and `pairwise` functions as above.\n",
    "* The $\\sigma_i$ are optimized in parallel.\n",
    "* Rows of the gradient are computed in parallel.\n",
    "\n",
    "We have used `np.allclose` to verify that different levels of optimization do indeed produce the same outputs (included in unit testing).\n",
    "\n",
    "### Benchmarking\n",
    "We first test each version of the three critical functions on simualted data. The following simulations are performed with $1000$ points of ambient data drawn from a $20$-dimensional standard normal and $1000$ points of embedded data drawn from a $3$ dimensional standard normal. The difference $p_{ij}-q_{ij}$ is drawn from a Gamma($1,1$) distribution. None of these distributional choices significantly influence the results; only the dimensions matter.\n",
    "\n",
    "First, we study the distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6cf0abf86ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Y_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dists' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(0, 1, [1000, 20])\n",
    "D = get_dists(X)\n",
    "R = np.random.gamma(1, 1, [1000, 1000])\n",
    "Y = np.random.normal(0, 1, [1000, 3])\n",
    "Y_dists = get_Y_dists(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optim in [\"none\", \"fastest\"]:\n",
    "    %timeit get_dists(X, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving from a double for loop to the optimized sklearn function results in <font color=red>**###**</font>-fold speed-up. This is a tremendous improvement, since the distance function is run at every iteration of t-SNE. Next we consider the effects of optimizing variances in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optim in [\"none\", \"fastest\"]:\n",
    "    %timeit get_P(D, optim = optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel computation produces <font color=red>**###**</font>-fold speed-up. The gains from this are only marginal, as we only need to optimize variances once upon initialization. Finally, we study the effects of vectorizing the gradient as well as computing rows in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optim in [\"none\", \"fast\", \"fastest\"]:\n",
    "    %timeit get_grad(R, Y_dists, Y, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization produces <font color=red>**###**</font>-fold speed-up and parallelization produces <font color=red>**###**</font>-fold speed-up. The gradient is computed at every iteration, so this is a major improvement. \n",
    "\n",
    "For completeness, we also run the full algorithm at all three levels of optimization. Note the slowest version of the algorithm is run for **only** 50 iterations, whereas the two fastest versions are run for $1000$ iterations. There are two reasons for this choice. First, the slowest version takes an hour for $1000$ iterations (hence $7$ hours to benchmark). Second, the gains from parallel computing are not visible for a small number of iterations due to the additional overhead of starting parallel processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tsne(X = X, niter = 50, optim = \"none\", verbose = False) \n",
    "%timeit tsne(X = X, niter = 1000, optim = \"fast\", verbose = False)\n",
    "%timeit tsne(X = X, niter = 1000, optim = \"fastest\", verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization produces <font color=red>**###**</font>-fold speed-up and parallelization produces <font color=red>**###**</font>-fold speed-up. A raw Python implementation of t-SNE would not be useful for analyzing large modern datasets - parallelization is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "**To be done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "### Simulated data [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that t-SNE output is influenced by a randomly chosen initial state, the exact same function inputs can yield vastly different results. Below, we show the output from 3 different (identical) runs of t-SNE, starting from three parallel lines with moderate spacing. This initial example is meant to illustrate that there do not exist specific inputs that give known outputs for t-SNE. However, we can produce datasets that have known grouping structures (denoted by separate colors) and observe how well t-SNE preserves these grouping structures. In this example, we note that the all of the outputs, though different in appearance, preserve the three lines in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labs = make_parallel_lines(spacing = 40, num_lines = 3)\n",
    "cdict = {1: 'red', 2: 'mediumspringgreen', 3: 'royalblue'}\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c = np.array(list(map(lambda x: cdict[x], labs))))\n",
    "plt.title(\"Initial state: 3 parallel lines\")\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_plots(X = X, labs = labs, perp_vec = [50, 50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we demonstrate how t-SNE performs across a range of perplexity values for four simple datasets: \n",
    "\n",
    "- Two clusters of 500-dimensional Gaussian data with equal variance (of which 2 dimensions are shown in the top row of the grid below)\n",
    "- Two clusters of 500-dimensional Gaussian data with unequal variance (of which 2 dimensions are shown in the top row of the grid below)\n",
    "- Two parallel lines with very little spacing between them\n",
    "- Three parallel lines with moderate spacing, which are the same lines that were used in the demonstration above\n",
    "\n",
    "From these examples, we can extract a few insights about the performance of t-SNE:\n",
    "\n",
    "- Given a perplexity value that is appropriate for a given dataset, t-SNE appears to be quite proficient at preserving clusters (both for high dimensional and low dimensional starting points)\n",
    "- Shape is not necessarily preserved through the t-SNE algorithm. For instance, t-SNE does not return two clusters of unequal size from the 500-dimensional Gaussians with unequal variance\n",
    "- t-SNE has success across a wide range of perplexity values for groups that can be easily separated (e.g., the Gaussian blobs). For datasets with less discernable separation (e.g., two tight parallel lines), the choice of perplexity value is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plots(data_list = [make_two_blobs(), make_two_blobs(blob1_sd = 1.5), make_parallel_lines(), \\\n",
    "                              make_parallel_lines(spacing = 40, num_lines = 3)], \\\n",
    "                 title_list = [\"500-D Gaussians, equal variance\", \\\n",
    "                               \"500-D Gaussians, unequal variance\", \\\n",
    "                               \"Two tight parallel lines\", \\\n",
    "                              \"Three spaced parallel lines\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the shapes considered above were quite simple, we also tested t-SNE on 3-dimensional objects with more complicated structures:\n",
    "\n",
    "- Linked circles\n",
    "- A trefoil knot\n",
    "- Intertwined springs\n",
    "\n",
    "While t-SNE is typically used for dimensionality reduction of high dimensional data, we chose more complicated structures in three dimensions due to ease of visualization. At low perplexity values, t-SNE does not perform well on these shapes. However, at moderate perplexity values, t-SNE appropriately flattens the 3-dimensional shapes into reasonable clusters, and at high perplexity values, more of the original structure is shown (e.g., the linking of the two circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plots(data_list = [make_two_3d_circles(), make_trefoil_knot(), make_springs()], \\\n",
    "                 title_list = [\"Linked circles in 3D\", \\\n",
    "                               \"Trefoil knot in 3D\", \\\n",
    "                               \"Intertwined springs in 3D\"], plot_3d = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we were interested in studying the evolution of the t-SNE algorithm as a function of the iteration number. At least for the linked circles and trefoil knot, we still see evidence of t-SNE's random starting point at step 10, and then the appropriate shapes quickly begin to form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plots(data_list = [make_two_3d_circles(), make_trefoil_knot(), make_springs()], \\\n",
    "                 title_list = [\"Linked circles in 3D\", \\\n",
    "                               \"Trefoil knot in 3D\", \\\n",
    "                               \"Intertwined springs in 3D\"], plot_3d = True, perp_plot = False, step_plot_perp = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data and Comparisons [6, 7, 8]\n",
    "\n",
    "**To be done**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "**TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions\n",
    "\n",
    "Overall, we all feel that this project has been a positive team experience and that all team members have contributed approximately equally. Initially, we all read the \"Visualizing High-Dimensional Data Using t-SNE\" paper and created three separate working versions of the algorithm. We then came together and pooled the best functions from our original versions, and there is no clear division or attibution for the main body of the code. \n",
    "\n",
    "Throughout the project, we frequently collaborated with each other and asked questions to ensure that team members felt supported and were not overburdened with work. That said, for the report, each of us took ownership over distinct parts, which are listed below:\n",
    "\n",
    "- Steven wrote the sections entitled `Background`, `Description of Algorithm`, and `Optimization`. Steven is also responsible for optimizing our code beyond the original pooled version and writing the code for the benchmarking. Lastly, Steven contributed to writing unit tests\n",
    "\n",
    "- Rob wrote all code and interpretations for the `Applications to Simulated Data` section of the report. Rob also contibuted to writing unit tests, published the package, maintained the Github repository, and documented references\n",
    "\n",
    "- Marc wrote all code and interpretations for the `Applications to Real Data` and `Comparison to Competing Algorithms` sections of the report. Marc also contibuted to writing unit tests \n",
    "\n",
    "All team members were responsible for reading and copy-editing the report in its entirety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package (`tsne663`) can be installed from [TestPyPI](https://test.pypi.org/project/tsne663/0.3/) using `pip install -i https://test.pypi.org/simple/ tsne663==0.3`. Alternatively, one can navigate to the [Github repository](https://github.com/robkravec/STA663_Project) and run `python setup.py install`. \n",
    "\n",
    "Upon installation, the following prerequisites will also be installed (versions shown are those used during development, which ensures that the package will work properly):\n",
    "\n",
    "- `matplotlib` - 3.1.1\n",
    "- `numpy` - 1.18.1\n",
    "- `numba` - 0.48.0\n",
    "- `tqdm` - 4.42.0\n",
    "- `sklearn` - 0.22.1\n",
    "\n",
    "Two additional packages, while not automatically installed, are necessary to reproduce the results from the simulations:\n",
    "\n",
    "- `plotly` - 4.5.0\n",
    "- `textwrap` - 3.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\] L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.\n",
    "\n",
    "\\[2\\] Vin de Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. Proccedings of the 15th International Convference on Neural Information Processing Systems. Jan 2002.\n",
    "\n",
    "\n",
    "\\[3\\] Dmitry Kobak, George Linderman, Stefan Steinerberger, Yuval Kluger, and Philipp Berens. Heavy-tailed kernels reveal a finer cluster structure in t-SNE visualisations. ECML PKDD 2019. doi: 10.1007/978-3-030-46150-8_8\n",
    "\n",
    "\\[4\\] L.J.P. van der Maaten. Apr 2021. https://lvdmaaten.github.io/tsne/.\n",
    "\n",
    "\\[5\\] Wattenberg, et al., \"How to Use t-SNE Effectively\", Distill, 2016. http://doi.org/10.23915/distill.00002\n",
    "\n",
    "\\[6\\] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST Database. Apr 2021. http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "\\[7\\] UCI Machine Learning Respository. Gene expression cancer RNA-seq Data Set. Jun 2016. https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#.\n",
    "\n",
    "\\[8\\] Allen Institute for Brain Science. Human M1 10x. Apr 2021. https://portal.brain-map.org/atlases-and-data/rnaseq/human-m1-10x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
